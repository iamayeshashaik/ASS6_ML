{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "SMS Spam Collection Dataset\n",
        "\n",
        "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
        "\n",
        "Dataset Link: https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
        "\n",
        "# Importing Necessary Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "df1 = pd.read_csv(\"D:\\\\Desktop\\\\DataSets\\\\spam.csv\",encoding=\"latin-1\")\n",
        "df1.head()\n",
        "v1\tv2\tUnnamed: 2\tUnnamed: 3\tUnnamed: 4\n",
        "0\tham\tGo until jurong point, crazy.. Available only ...\tNaN\tNaN\tNaN\n",
        "1\tham\tOk lar... Joking wif u oni...\tNaN\tNaN\tNaN\n",
        "2\tspam\tFree entry in 2 a wkly comp to win FA Cup fina...\tNaN\tNaN\tNaN\n",
        "3\tham\tU dun say so early hor... U c already then say...\tNaN\tNaN\tNaN\n",
        "4\tham\tNah I don't think he goes to usf, he lives aro...\tNaN\tNaN\tNaN\n",
        "print('shape of DS : \\n', df1.shape,'\\n', '-'*100, sep='')\n",
        "print('Info of DS : \\n',df1.info(),'\\n', '-'*100, sep='' )\n",
        "print('Description of DS : \\n',df1.describe(),'\\n', '-'*100, sep='' )"
      ],
      "metadata": {
        "id": "lK-00U0kwYS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape of DS : \n",
        "(5572, 5)\n",
        "----------------------------------------------------------------------------------------------------\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 5572 entries, 0 to 5571\n",
        "Data columns (total 5 columns):\n",
        " #   Column      Non-Null Count  Dtype \n",
        "---  ------      --------------  ----- \n",
        " 0   v1          5572 non-null   object\n",
        " 1   v2          5572 non-null   object\n",
        " 2   Unnamed: 2  50 non-null     object\n",
        " 3   Unnamed: 3  12 non-null     object\n",
        " 4   Unnamed: 4  6 non-null      object\n",
        "dtypes: object(5)\n",
        "memory usage: 217.8+ KB\n",
        "Info of DS : \n",
        "None\n",
        "----------------------------------------------------------------------------------------------------\n",
        "Description of DS : \n",
        "          v1                      v2  \\\n",
        "count   5572                    5572   \n",
        "unique     2                    5169   \n",
        "top      ham  Sorry, I'll call later   \n",
        "freq    4825                      30   \n",
        "\n",
        "                                               Unnamed: 2  \\\n",
        "count                                                  50   \n",
        "unique                                                 43   \n",
        "top      bt not his girlfrnd... G o o d n i g h t . . .@\"   \n",
        "freq                                                    3   \n",
        "\n",
        "                   Unnamed: 3 Unnamed: 4  \n",
        "count                      12          6  \n",
        "unique                     10          5  \n",
        "top      MK17 92H. 450Ppw 16\"    GNT:-)\"  \n",
        "freq                        2          2  "
      ],
      "metadata": {
        "id": "ED5GdFRwwq-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1\n"
      ],
      "metadata": {
        "id": "dWB3S3jswyQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "df1.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)\n",
        "df1.rename(columns={'v1':'target','v2':'text'},inplace=True)\n",
        "df1 = df1.drop_duplicates(keep='first')\n",
        "encoder = LabelEncoder()\n",
        "df1['target'] = encoder.fit_transform(df1['target'])\n",
        "df1.head()\n",
        "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_8488\\3512318194.py:5: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  df1['target'] = encoder.fit_transform(df1['target'])"
      ],
      "metadata": {
        "id": "3Uh3sjvnw4nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{df1.isnull().sum()}\\nNum duplicates: {df1.duplicated().sum()}')\n",
        "df1.shape\n",
        "target    0\n",
        "text      0\n",
        "dtype: int64\n",
        "Num duplicates: 0"
      ],
      "metadata": {
        "id": "q1KFKEtLxAhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1['target'].value_counts())\n",
        "plt.pie(df1['target'].value_counts(), labels=['ham','spam'],autopct=\"%0.2f\")\n",
        "plt.show()\n",
        "0    4516\n",
        "1     653\n",
        "Name: target, dtype: int64\n",
        "\n",
        "import nltk"
      ],
      "metadata": {
        "id": "mU5m6VK7xGqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['num_characters'] = df1['text'].apply(len)\n",
        "df1['num_words'] = df1['text'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
        "df1['num_sentences'] = df1['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
        "df1.head()\n",
        "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_8488\\1157530986.py:1: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  df1['num_characters'] = df1['text'].apply(len)\n",
        "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_8488\\1157530986.py:2: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  df1['num_words'] = df1['text'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
        "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_8488\\1157530986.py:3: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  df1['num_sentences'] = df1['text'].apply(lambda x:len(nltk.sent_tokenize(x)))"
      ],
      "metadata": {
        "id": "sJQK_F0uxNsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[df1['target'] == 1][['num_characters','num_words','num_sentences']].describe()"
      ],
      "metadata": {
        "id": "owUiEYuYxVAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(df1,hue='target')"
      ],
      "metadata": {
        "id": "AfCS8XipxbZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(data=df1.corr(), annot=True,cmap=\"coolwarm\", center=0, ax=plt.subplots(figsize=(8,6))[1])\n",
        "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_8488\\4239310010.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
        "  sns.heatmap(data=df1.corr(), annot=True,cmap=\"coolwarm\", center=0, ax=plt.subplots(figsize=(8,6))[1])"
      ],
      "metadata": {
        "id": "LhyRBzYJxgF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet as wn \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "[nltk_data] Downloading package punkt to\n",
        "[nltk_data]     C:\\Users\\tharu\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!\n",
        "[nltk_data] Downloading package stopwords to\n",
        "[nltk_data]     C:\\Users\\tharu\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package stopwords is already up-to-date!\n",
        "[nltk_data] Downloading package wordnet to\n",
        "[nltk_data]     C:\\Users\\tharu\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package wordnet is already up-to-date!"
      ],
      "metadata": {
        "id": "YD68JHZMxmdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, KFold, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "from keras.layers import LeakyReLU,PReLU,ELU\n",
        "from keras.layers import Dropout\n",
        "\n",
        "\n",
        "import pickle\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "oVHJFcHUxsx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[nltk_data] Downloading package punkt to\n",
        "[nltk_data]     C:\\Users\\tharu\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!\n",
        "[nltk_data] Downloading package stopwords to\n",
        "[nltk_data]     C:\\Users\\tharu\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package stopwords is already up-to-date!\n",
        "def transform_text(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    \n",
        "    tokens = [t for t in tokens if t.isalnum()]\n",
        "    \n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    punctuation_set = set(string.punctuation)\n",
        "    tokens = [t for t in tokens if t not in stopwords_set and t not in punctuation_set]\n",
        "    \n",
        "    ps = PorterStemmer()\n",
        "    tokens = [ps.stem(t) for t in tokens]\n",
        "    \n",
        "    return \" \".join(tokens)\n",
        "print(f\"{df1['text'][10]}\\n{transform_text(df1['text'][10])}\")\n",
        "df1['transformed_text'] = df1['text'].apply(transform_text)\n",
        "df1.head()\n",
        "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
        "gon na home soon want talk stuff anymor tonight k cri enough today\n",
        "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_8488\\2134138213.py:2: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
        "  df1['transformed_text'] = df1['text'].apply(transform_text)"
      ],
      "metadata": {
        "id": "R8gdTwHix0mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')\n",
        "spam_wc = wc.generate(df1[df1['target'] == 1]['transformed_text'].str.cat(sep=\" \"))\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.imshow(spam_wc)"
      ],
      "metadata": {
        "id": "DyGLmBCSx7X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_corpus = []\n",
        "for msg in df1[df1['target'] == 1]['transformed_text'].tolist():\n",
        "    for word in msg.split():\n",
        "        spam_corpus.append(word)\n",
        "\n",
        "ham_corpus = []\n",
        "for msg in df1[df1['target'] == 0]['transformed_text'].tolist():\n",
        "    for word in msg.split():\n",
        "        ham_corpus.append(word)\n",
        "  \n",
        "print(len(spam_corpus), len(ham_corpus))\n",
        "9939 35404\n",
        "# RNN\n",
        "\n",
        "X=df1['transformed_text']\n",
        "y = df1['target'].values\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "tok = Tokenizer(num_words=max_words)\n",
        "tok.fit_on_texts(X_train)\n",
        "sequences = tok.texts_to_sequences(X_train)\n",
        "sequences_matrix = pad_sequences(sequences,maxlen=max_len)\n",
        "def RNN():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    layer = LSTM(64)(layer)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "FSc9y7ZkyB68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN()\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " inputs (InputLayer)         [(None, 150)]             0         \n",
        "                                                                 \n",
        " embedding (Embedding)       (None, 150, 50)           50000     \n",
        "                                                                 \n",
        " lstm (LSTM)                 (None, 64)                29440     \n",
        "                                                                 \n",
        " FC1 (Dense)                 (None, 256)               16640     \n",
        "                                                                 \n",
        " activation (Activation)     (None, 256)               0         \n",
        "                                                                 \n",
        " dropout (Dropout)           (None, 256)               0         \n",
        "                                                                 \n",
        " out_layer (Dense)           (None, 1)                 257       \n",
        "                                                                 \n",
        " activation_1 (Activation)   (None, 1)                 0         \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 96,337\n",
        "Trainable params: 96,337\n",
        "Non-trainable params: 0"
      ],
      "metadata": {
        "id": "r7jr4FToyI2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(sequences_matrix,y_train,batch_size=128,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
        "Epoch 1/10\n",
        "26/26 [==============================] - 8s 187ms/step - loss: 0.3848 - accuracy: 0.8492 - val_loss: 0.1964 - val_accuracy: 0.9057\n",
        "Epoch 2/10\n",
        "26/26 [==============================] - 8s 290ms/step - loss: 0.1482 - accuracy: 0.9580 - val_loss: 0.0746 - val_accuracy: 0.9782\n",
        "<keras.callbacks.History at 0x1c6bd74b520>\n",
        "test_sequences = tok.texts_to_sequences(X_test)\n",
        "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)\n",
        "accr = model.evaluate(test_sequences_matrix,y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "33/33 [==============================] - 0s 13ms/step - loss: 0.0907 - accuracy: 0.9700\n",
        "Test set\n",
        "  Loss: 0.091\n",
        "  Accuracy: 0.970\n",
        "CommonLit Readability Prize\n",
        "\n",
        "Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.\n",
        "\n",
        "In this competition, we're predicting the reading ease of excerpts from literature. We've provided excerpts from several time periods and a wide range of reading ease scores. Note that the test set includes a slightly larger proportion of modern texts (the type of texts we want to generalize to) than the training set.\n",
        "\n",
        "Dataset Link: https://www.kaggle.com/c/commonlitreadabilityprize/data"
      ],
      "metadata": {
        "id": "uF38PZT0yPMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "test_data = pd.read_csv(\"D:\\\\Desktop\\\\DataSets\\\\CommonLit\\\\test.csv\")\n",
        "train_data = pd.read_csv(\"D:\\\\Desktop\\\\DataSets\\\\CommonLit\\\\train.csv\")\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "haOAE3FuyVC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()\n",
        "id\turl_legal\tlicense\texcerpt\n",
        "0\tc0f722661\tNaN\tNaN\tMy hope lay in Jack's promise that he would ke...\n",
        "1\tf0953f0a5\tNaN\tNaN\tDotty continued to go to Mrs. Gray's every nig...\n",
        "2\t0df072751\tNaN\tNaN\tIt was a bright and cheerful scene that greete...\n",
        "3\t04caf4e0c\thttps://en.wikipedia.org/wiki/Cell_division\tCC BY-SA 3.0\tCell division is the process by which a parent...\n",
        "4\t0e63f8bea\thttps://en.wikipedia.org/wiki/Debugging\tCC BY-SA 3.0\tDebugging is the process of finding and resolv...\n",
        "train_data = train_data[['excerpt']]\n",
        "test_data = test_data[['excerpt']]\n",
        "train_data['excerpt_lower'] = train_data['excerpt'].str.lower()\n",
        "test_data['excerpt_lower'] = test_data['excerpt'].str.lower()\n",
        "train_data.head()\n",
        "test_data.head()\n",
        "excerpt\texcerpt_lower\n",
        "0\tMy hope lay in Jack's promise that he would ke...\tmy hope lay in jack's promise that he would ke...\n",
        "1\tDotty continued to go to Mrs. Gray's every nig...\tdotty continued to go to mrs. gray's every nig...\n",
        "2\tIt was a bright and cheerful scene that greete...\tit was a bright and cheerful scene that greete...\n",
        "3\tCell division is the process by which a parent...\tcell division is the process by which a parent...\n",
        "4\tDebugging is the process of finding and resolv...\tdebugging is the process of finding and resolv...\n",
        "import requests\n",
        "\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.text\n",
        "print(content[2745:3948])\n",
        " <body> level - not in <div> or <table>  */\n",
        "    text-align: justify;\n",
        "    /* or left?? */\n",
        "    text-indent: 1em;\n",
        "    /* first-line indent */\n",
        "    }\n",
        "/* suppress indentation on paragraphs following heads  */\n",
        "h2 + p, h3 + p, h4 + p {\n",
        "    text-indent: 0\n",
        "    }\n",
        "/* tighter spacing for list item paragraphs */\n",
        "dd, li {\n",
        "    margin-top: 0.25em;\n",
        "    margin-bottom: 0;\n",
        "    line-height: 1.2em;\n",
        "    /* a bit closer than p's */\n",
        "    }\n",
        "/* ************************************************************************\n",
        " * Head 2 is for chapter heads. \n",
        " * ********************************************************************** */\n",
        "h2 {\n",
        "    /* text-align:center;  left-aligned by default. */\n",
        "    margin-top: 3em;\n",
        "    /* extra space above.. */\n",
        "    margin-bottom: 2em;\n",
        "    /* ..and below */\n",
        "    clear: both;\n",
        "    /* don't let sidebars overlap */\n",
        "    }\n",
        "/* ************************************************************************\n",
        " * Head 3 is for main-topic heads.\n",
        " * ********************************************************************** */\n",
        "h3 {\n",
        "    /* text-align:center;  left-aligned by default. */\n",
        "    margin-top: 2em;\n",
        "    /* extra space above but not below */\n",
        "    font-weight: normal;\n",
        "    /* ove\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    \"\"\"\n",
        "    This function will remove the HTML tags & noise from the scraped data.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[1163:1957])\n",
        " darkness was\n",
        "           upon the face of the deep. And the Spirit of God moved upon\n",
        "           the face of the waters.\n",
        "01:001:003 And God said, Let there be light: and there was light.\n",
        "01:001:004 And God saw the light, that it was good: and God divided the\n",
        "           light from the darkness.\n",
        "01:001:005 And God called the light Day, and the darkness he called\n",
        "           Night. And the evening and the morning were the first day.\n",
        "01:001:006 And God said, Let there be a firmament in the midst of the\n",
        "           waters, and let it divide the waters from the waters.\n",
        "01:001:007 And God made the firmament, and divided the waters which were\n",
        "           under the firmament from the waters which were above the\n",
        "           firmament: and it was so.\n",
        "01:001:008 And God called the firmament Heaven. A\n",
        "train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(strip_html_tags)\n",
        "test_data['excerpt_lower'] = test_data['excerpt_lower'].apply(strip_html_tags)\n",
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "s = 'S√≥mƒõ √Åccƒõntƒõd tƒõxt'\n",
        "remove_accented_chars(s)\n",
        "'Some Accented text'\n",
        "train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(remove_accented_chars)\n",
        "test_data['excerpt_lower'] = test_data['excerpt_lower'].apply(remove_accented_chars)\n",
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ üôÇüôÇüôÇ\"\n",
        "remove_special_characters(s, remove_digits=False)\n",
        "'Well this was fun See you at 730 What do you think 9318 '\n",
        "remove_special_characters(s)\n",
        "'Well this was fun See you at  What do you think  '\n",
        "train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(remove_special_characters)\n",
        "test_data['excerpt_lower'] = test_data['excerpt_lower'].apply(remove_special_characters)\n",
        "s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
        "import contractions\n",
        "list(contractions.contractions_dict.items())[:10]\n",
        "[(\"I'm\", 'I am'),\n",
        " (\"I'm'a\", 'I am about to'),\n",
        " (\"I'm'o\", 'I am going to'),\n",
        " (\"I've\", 'I have'),\n",
        " (\"I'll\", 'I will'),\n",
        " (\"I'll've\", 'I will have'),\n",
        " (\"I'd\", 'I would'),\n",
        " (\"I'd've\", 'I would have'),\n",
        " ('Whatcha', 'What are you'),\n",
        " (\"amn't\", 'am not')]"
      ],
      "metadata": {
        "id": "intxiiipyaZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions.fix(s)\n",
        "'You all cannot expand contractions I would think! You would not be able to. How did you do it?'\n",
        "train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(contractions.fix)\n",
        "test_data['excerpt_lower'] = test_data['excerpt_lower'].apply(contractions.fix)\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')\n",
        "('jump', 'jump', 'jump')\n",
        "ps.stem('lying')\n",
        "'lie'\n",
        "ps.stem('strange')\n",
        "'strang'\n",
        "train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(ps.stem)\n",
        "test_data['excerpt_lower'] = test_data['excerpt_lower'].apply(ps.stem)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "print(wnl.lemmatize('cars', 'n'))\n",
        "print(wnl.lemmatize('boxes', 'n'))\n",
        "car\n",
        "box\n",
        "train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(wnl.lemmatize)\n",
        "test_data['excerpt_lower'] = test_data['excerpt_lower'].apply(wnl.lemmatize)\n",
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "tokens = nltk.word_tokenize(s)\n",
        "print(tokens)\n",
        "['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n",
        "train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(nltk.word_tokenize)\n",
        "test_data['excerpt_lower'] = test_data['excerpt_lower'].apply(nltk.word_tokenize)\n",
        "from nltk.corpus import stopwords\n",
        "\", \".join(stopwords.words('english'))\n",
        "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\"\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "train_data[\"excerpt_lower\"] = train_data[\"excerpt_lower\"].apply(lambda text: remove_stopwords(text))\n",
        "test_data[\"excerpt_lower\"] = test_data[\"excerpt_lower\"].apply(lambda text: remove_stopwords(text))\n",
        "Reddit Comment Score Prediction\n",
        "\n",
        "Reddit is a social news platform that allows users to discuss and vote on content that other users have submitted.On an average reddit receives 470,000 comments per day. The comments are further upvoted or downvoted by the registered users.\n",
        "\n",
        "Imagine you are going to start a forum where users can post or comment or share content on the platform. Now you want to filter out some positive comments and recommend them to your users.\n",
        "\n",
        "Build a machine learning model that will help you know which comment or content is going to be popular in the near future (the content which receives the highest upvotes will be popular) and accordingly recommend such content to your users.\n",
        "\n",
        "Dataset Link: https://dphi.tech/challenges/data-sprint-36-reddit-comment-score-prediction/89/data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "df = pd.read_csv(\"D:\\Desktop\\DataSets\\Reddit\\Train_Data.csv\")\n",
        "print(df.shape)\n",
        "df.head()\n",
        "(4999, 9)"
      ],
      "metadata": {
        "id": "XgXbTcn-yqmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 4999 entries, 0 to 4998\n",
        "Data columns (total 9 columns):\n",
        " #   Column                   Non-Null Count  Dtype \n",
        "---  ------                   --------------  ----- \n",
        " 0   text                     4999 non-null   object\n",
        " 1   author                   4999 non-null   object\n",
        " 2   controversiality         4999 non-null   int64 \n",
        " 3   parent_text              4999 non-null   object\n",
        " 4   parent_score             4999 non-null   int64 \n",
        " 5   parent_votes             4999 non-null   int64 \n",
        " 6   parent_author            4999 non-null   object\n",
        " 7   parent_controversiality  4999 non-null   int64 \n",
        " 8   Score                    4999 non-null   int64 \n",
        "dtypes: int64(5), object(4)\n",
        "memory usage: 351.6+ KB\n",
        "cols = ['text', 'author', 'controversiality', 'parent_text', 'parent_score', 'parent_votes',\n",
        "'parent_author', 'parent_controversiality', 'Score']\n",
        "for col in cols:\n",
        "    print(col,':',df[col].nunique())\n",
        "text : 4992\n",
        "author : 4317\n",
        "controversiality : 2\n",
        "parent_text : 4992\n",
        "parent_score : 1001\n",
        "parent_votes : 1001\n",
        "parent_author : 4448\n",
        "parent_controversiality : 2\n",
        "Score : 572\n",
        "df['score vs. votes'] = df['parent_score']==df['parent_votes']\n",
        "df['score vs. votes'].nunique()\n",
        "1\n",
        "df.drop(['parent_votes', 'score vs. votes'], axis= 1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JXTHkdjXzEvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cor = df.corr()\n",
        "sns.heatmap(cor)\n",
        "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_8488\\2380579405.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
        "  cor = df.corr()"
      ],
      "metadata": {
        "id": "WW3bzxd1zGNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['text','author','parent_text','parent_author']\n",
        "for col in df[categorical_cols]:\n",
        "    df[col] = df[col].str.lower()\n",
        "    df[col] = df[col].str.strip()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "UbrrtvNAzMRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "for col in df[categorical_cols]:\n",
        "    df[col] = df[col].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "g4-qd42xzRd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def text_tokens(row):\n",
        "    text = row['text']\n",
        "    tokens = word_tokenize(text)\n",
        "    # taken only words (not punctuation)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words\n",
        "    df['text_tokens'] = df.apply(text_tokens, axis=1)\n",
        "\n",
        "def parent_text_tokens(row):\n",
        "    parent_text = row['parent_text']\n",
        "    tokens = word_tokenize(parent_text)\n",
        "    # taken only words (not punctuation)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words\n",
        "    df['parent_text_tokens'] = df.apply(parent_text_tokens, axis=1)\n",
        "df.head()\n",
        "[nltk_data] Downloading package stopwords to\n",
        "[nltk_data]     C:\\Users\\tharu\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package stopwords is already up-to-date!\n",
        "[nltk_data] Downloading package punkt to\n",
        "[nltk_data]     C:\\Users\\tharu\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!"
      ],
      "metadata": {
        "id": "mBUhhJ6qzV5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "tokens_cols = ('text_tokens', 'parent_text_tokens')\n",
        "\n",
        "for col in tokens_cols:\n",
        "    df[col] = df[col].apply(lambda x: ' '.join([w for w in x if w not in (stop_words)]))\n",
        "df.head()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatize_function = lambda x: [lemmatizer.lemmatize(str(word)) for word in x.split()]\n",
        "\n",
        "for col in tokens_cols:\n",
        "    df[col] = df[col].apply(lemmatize_function)\n",
        "df.head()\n",
        "df['text']= df['text_tokens'].apply(lambda x: ' '.join(x))\n",
        "df['parent_text']= df['parent_text_tokens'].apply(lambda x: ' '.join(x))\n",
        "df.drop(['text_tokens', 'parent_text_tokens'], axis=1, inplace= True)\n",
        "\n",
        "df.head()\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "text = vectorizer.fit_transform(df['text']).toarray()\n",
        "text = pd.DataFrame(text, columns=vectorizer.get_feature_names())\n",
        "\n",
        "text.shape"
      ],
      "metadata": {
        "id": "lEzaPcqgzcCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer1 = TfidfVectorizer(max_features=50,min_df=1,max_df=0.7)\n",
        "text_tf_idf = vectorizer1.fit_transform(df['text']).toarray()\n",
        "text_tf_idf = pd.DataFrame(text_tf_idf, columns=vectorizer1.get_feature_names())\n",
        "\n",
        "text_tf_idf.shape\n",
        "num_cols = df[['controversiality', 'parent_score', 'parent_controversiality']]\n",
        "x = pd.concat([text_tf_idf, num_cols], axis=1)\n",
        "y = df['Score']\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the data\n",
        "x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2, random_state = 42)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "pred_y = lr.predict(x_val)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print('Root Mean Squared Error is: ', np.sqrt(mean_squared_error(y_val, pred_y)))\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgbReg = XGBRegressor(verbosity=0)\n",
        "xgbReg.fit(x_train, y_train)\n",
        "\n",
        "pred_y1 = xgbReg.predict(x_val)\n",
        "\n",
        "# Root mean squared error\n",
        "print('Root Mean Squared Error is: ', np.sqrt(mean_squared_error(y_val, pred_y1)))\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter for n_neighbors\n",
        "param_grid = {'n_neighbors': np.arange(1, 100)}\n",
        "knn = KNeighborsRegressor()\n",
        "knn_cv = GridSearchCV(knn, param_grid, cv=5)\n",
        "knn_cv.fit(x_train, y_train)\n",
        "knn_cv.best_params_\n",
        "knn = KNeighborsRegressor(n_neighbors = 50)\n",
        "knn.fit(x_train, y_train)\n",
        "\n",
        "pred_y2 = knn.predict(x_val)\n",
        "\n",
        "# Root mean squared error\n",
        "print('Root Mean Squared Error is: ', np.sqrt(mean_squared_error(y_val, pred_y2)))\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "param_grid = {'n_estimators': np.arange(1, 50), 'max_depth': np.arange(1, 50)}\n",
        "\n",
        "RFRegressor = RandomForestRegressor()\n",
        "RFRegressor_cv = GridSearchCV(RFRegressor, param_grid, cv=5)\n",
        "RFRegressor_cv.fit(x_train, y_train)\n",
        "RFRegressor_cv.best_params_\n",
        "randForest = RandomForestRegressor(n_estimators=9, max_depth=3, max_features='auto')\n",
        "randForest.fit(x_train, y_train)\n",
        "\n",
        "pred_y3 = randForest.predict(x_val)\n",
        "\n",
        "print('Root Mean Squared Error is: ', np.sqrt(mean_squared_error(y_val, pred_y3)))\n",
        "test_data = pd.read_csv('Test_Data.csv')\n",
        "test_data.head()\n",
        "test_data['text'] = test_data['text'].str.lower()\n",
        "test_data['text'] = test_data['text'].str.strip()\n",
        "\n",
        "test_data['text'] = test_data['text'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
        "\n",
        "test_data['text_tokens'] = df.apply(text_tokens, axis=1)\n",
        "\n",
        "test_data['text_tokens'] = test_data['text_tokens'].apply(lambda x: ' '.join([w for w in x if w not in (stop_words)]))\n",
        "\n",
        "test_data['text_tokens'] = test_data['text_tokens'].apply(lemmatize_function)\n",
        "\n",
        "test_data['text']= test_data['text_tokens'].apply(lambda x: ' '.join(x))\n",
        "test_data.drop(['text_tokens'], axis=1, inplace= True)\n",
        "\n",
        "text_data_tf_idf = vectorizer1.fit_transform(test_data['text']).toarray()\n",
        "test_num = test_data[['controversiality','parent_score', 'parent_controversiality']]\n",
        "\n",
        "test = pd.concat([text_data_tf_idf, test_num], axis=1)\n",
        "predict_test_y = lr.predict(test)\n",
        "predict_test_y1 = xgbReg.predict(test)\n",
        "predict_test_y2 = knn.predict(test)\n",
        "predict_test_y3 = randForest.predict(test)\n",
        "res = pd.DataFrame(predict_test_y2) \n",
        "res.index = test_data.index\n",
        "res.columns = [\"prediction\"]\n",
        "res.to_csv(\"submission6.csv\")"
      ],
      "metadata": {
        "id": "rQ-gmVYFziU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}